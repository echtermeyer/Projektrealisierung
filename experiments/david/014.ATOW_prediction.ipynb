{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\n"
     ]
    }
   ],
   "source": [
    "# working dir\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root_dir = os.path.dirname(os.path.dirname(cwd))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display Dataframe (with scrollbars)\n",
    "def ddf(df, max_height=500, max_width=1500):\n",
    "    \"\"\"\n",
    "    Display a pandas DataFrame with horizontal and vertical scrollbars in a Jupyter notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to display.\n",
    "    max_height (int): The maximum height of the scrollable area in pixels.\n",
    "    max_width (int): The maximum width of the scrollable area in pixels.\n",
    "    \"\"\"\n",
    "    style = f\"\"\"\n",
    "    <style>\n",
    "    .scrollable-dataframe {{\n",
    "        max-height: {max_height}px;\n",
    "        max-width: {max_width}px;\n",
    "        overflow: auto;\n",
    "        display: inline-block;\n",
    "        position: relative;\n",
    "    }}\n",
    "    .scrollable-dataframe thead th {{\n",
    "        position: sticky;\n",
    "        top: 0;\n",
    "        background-color: white;\n",
    "        z-index: 1;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    html = style + df.to_html(classes='scrollable-dataframe')\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_17680\\3172802451.py:2: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(\"../../src/data/ABCD_tripfiles_preprocessed.csv\"),\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_17680\\3172802451.py:3: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24,25,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(\"../../src/data/MNOP_tripfiles_preprocessed.csv\"),\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_17680\\3172802451.py:4: DtypeWarning: Columns (26,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(\"../../src/data/ZYXW_tripfiles_preprocessed.csv\"),\n"
     ]
    }
   ],
   "source": [
    "main = pd.concat([\n",
    "    pd.read_csv(\"../../src/data/ABCD_tripfiles_preprocessed.csv\"),\n",
    "    pd.read_csv(\"../../src/data/MNOP_tripfiles_preprocessed.csv\"),\n",
    "    pd.read_csv(\"../../src/data/ZYXW_tripfiles_preprocessed.csv\"),\n",
    "])\n",
    "\n",
    "meta_cols = ['flight_id', 'id', 'creation_time', 'airline_code', 'flight_date', 'action_name',]\n",
    "action_cols = ['departureAirport', 'departureTime', 'arrivalAirport', 'arrivalTime', 'aircraftRegistration', 'aircraftSubtype', 'aircraftVersion',]\n",
    "\n",
    "main = main.loc[\n",
    "    main[\"action_name\"] == \"CalculateWeightAndTrimAction\",\n",
    "    meta_cols + action_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable future deprecation warning of .fillna() method\n",
    "\n",
    "def fill_na_within_group(group):\n",
    "    group[action_cols] = group[action_cols].fillna(method='bfill')\n",
    "    group[action_cols] = group[action_cols].fillna(method='ffill')\n",
    "    return group\n",
    "\n",
    "main = main.groupby(\"flight_id\").apply(fill_na_within_group).reset_index(drop=True) # replaces NaN values with the values from the previous or next row within the same flight\n",
    "main.dropna(inplace=True) # drops 203828 rows - these are likely flights without an ASMMsgProcessor action\n",
    "\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_year(date_str):\n",
    "    date_part, time_part = date_str.split('T')\n",
    "    year, month, day = date_part.split('-')\n",
    "    hours, minutes, seconds = time_part.split(':')\n",
    "\n",
    "    # Correct year\n",
    "    if len(year) != 4:\n",
    "        year = year[1:]\n",
    "    \n",
    "    # Correct the minutes\n",
    "    if len(minutes) > 2:\n",
    "        minutes = minutes[:2]\n",
    "\n",
    "    # Correct the seconds\n",
    "    if len(seconds) > 6:\n",
    "        seconds = \"00.000Z\"\n",
    "    \n",
    "    date_part = f\"{year}-{month}-{day}\"\n",
    "    time_part = f\"{hours}:{minutes}:{seconds}\"\n",
    "    date_str = f\"{date_part}T{time_part}\"\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "# Fix year in departureTime and arrivalTime\n",
    "main[\"departureTime\"] = main[\"departureTime\"].apply(lambda x: correct_year(str(x)))\n",
    "main[\"arrivalTime\"] = main[\"arrivalTime\"].apply(lambda x: correct_year(str(x)))\n",
    "\n",
    "\n",
    "main[\"creation_time\"] = pd.to_datetime(main[\"creation_time\"])\n",
    "main[\"departureTime\"] = pd.to_datetime(main[\"departureTime\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "main[\"arrivalTime\"] = pd.to_datetime(main[\"arrivalTime\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "main[\"minutes_till_dep\"] = (main[\"departureTime\"] - main[\"creation_time\"]).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwata = pd.concat([\n",
    "    pd.read_csv(\"../../src/data/extracted/abcd_CalculateWeightAndTrimAction.csv\"), \n",
    "    pd.read_csv(\"../../src/data/extracted/mnop_CalculateWeightAndTrimAction.csv\"), \n",
    "    pd.read_csv(\"../../src/data/extracted/zyxw_CalculateWeightAndTrimAction.csv\")\n",
    "    ])\n",
    "\n",
    "cwata = cwata[[\n",
    "    'id', 'START_WI_weight', \n",
    "    'DO_WI_weight', 'PAX_WI_weight', 'TOTAL_DEADLOAD_WI_weight', 'TOTAL_LOAD_WI',\n",
    "    'TOTAL_TRAFFIC_LOAD', 'AZFW', 'ATOW', 'ALAW', 'ATXW',\n",
    "    'LIZFW', 'LITOW', 'LILAW',\n",
    "    'DEADLOAD_MAC', 'UNDERLOAD',\n",
    "    'ALLOWED_TOW', 'ALLOWED_ZFW', 'ALLOWED_LAW',\n",
    "    'ALLOWED_TXW',\n",
    "    'ESTIMATED_TRAFFIC_LOAD', 'ESTIMATED_ZFW',\n",
    "    'DELTA_ZFW'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(cwata, main, on=\"id\", how=\"left\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.sort_values(by=[\"flight_id\", \"creation_time\", \"id\"], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "azfw_all = df.groupby('flight_id')['ATOW'].last()\n",
    "azfw_all = azfw_all.to_dict()\n",
    "\n",
    "df[\"target_ATOW\"] = df[\"flight_id\"].map(azfw_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cols = ['flight_id', 'creation_time', 'id', 'action_name', \"departureTime\", \"arrivalTime\"]\n",
    "cat_cols = [\"airline_code\", \"departureAirport\", \"arrivalAirport\", \"aircraftRegistration\", \"aircraftSubtype\", \"aircraftVersion\"]\n",
    "target_col = \"target_ATOW\"\n",
    "num_cols = list(set(df.columns) - set(cat_cols) - set(special_cols) - {target_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data (dft = dataframe transformed)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "dft = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dft[num_cols] = scaler.fit_transform(dft[num_cols])\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "dft[target_col] = target_scaler.fit_transform(dft[[target_col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Separate features and target\n",
    "X = dft.drop(columns=[target_col] + special_cols)\n",
    "y = dft[target_col]\n",
    "\n",
    "# Change one hot bools to floats\n",
    "X = X.astype(float)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     f\"X shape: {X_tensor.shape}\",\n",
    "#     f\"y shape: {y_tensor.shape}\",\n",
    "#     f\"Feature variance:\\n{X.var()}\",\n",
    "#     f\"Target variance:\\n{y.var()}\",\n",
    "#     sep=\"\\n\"\n",
    "# )\n",
    "\n",
    "# #> Looking good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Regression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Regression, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_tensor.shape[1] # 620\n",
    "# model = SimpleNN(input_size)\n",
    "model = Regression(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# model.apply(weights_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "fc.weight tensor([[ 3.4475e-02, -2.0496e-02,  3.5146e-03,  1.8507e-02,  2.8835e-02,\n",
      "          2.5805e-02, -3.6599e-02, -4.5504e-03, -2.3241e-02,  3.5710e-03,\n",
      "          9.6225e-03,  2.1764e-03, -3.9652e-02,  2.4956e-03, -1.9120e-02,\n",
      "          2.0348e-02, -3.2615e-02,  2.9448e-02,  2.1593e-02,  2.3275e-02,\n",
      "         -1.7564e-02, -3.7917e-02,  2.1190e-02, -2.1341e-02, -1.7831e-02,\n",
      "          2.3429e-02,  1.1258e-02,  3.5971e-03,  1.6428e-02,  7.7486e-03,\n",
      "          5.6097e-04,  1.8416e-02,  1.1188e-02,  3.1521e-02, -1.6455e-02,\n",
      "         -3.5281e-02, -3.6845e-02, -2.2806e-02, -3.5456e-03, -7.7592e-03,\n",
      "          3.0267e-02,  6.7069e-03,  8.6650e-03,  1.3783e-02, -2.0547e-02,\n",
      "         -3.8929e-02, -1.1527e-02, -3.3035e-03, -3.2195e-02,  2.5192e-02,\n",
      "         -1.1705e-02, -3.4028e-02,  1.7513e-02, -1.8764e-02,  1.4112e-02,\n",
      "          2.6227e-02,  3.5817e-02, -2.3400e-02,  1.0692e-02,  4.2719e-03,\n",
      "          2.4125e-02, -2.2551e-02,  3.6035e-02, -6.5121e-03,  2.7966e-03,\n",
      "         -3.4685e-02, -1.4586e-02, -1.6327e-02, -3.8937e-02,  2.4363e-02,\n",
      "         -6.2360e-03, -2.1964e-02, -2.7446e-02,  1.6557e-02, -8.0024e-03,\n",
      "         -1.6390e-03,  2.9494e-02, -7.5255e-03, -2.9028e-02, -3.7465e-02,\n",
      "          6.5625e-03,  7.7407e-04, -3.8020e-02,  3.8995e-02, -3.9749e-02,\n",
      "         -3.5208e-02, -1.3830e-03,  3.2087e-02, -3.4006e-03, -9.7216e-03,\n",
      "          3.9931e-02, -1.5083e-02, -3.7625e-02, -2.0221e-02,  4.7113e-04,\n",
      "          5.9755e-04, -1.8200e-02, -1.0208e-02,  1.4473e-02,  3.5562e-02,\n",
      "          2.5674e-02, -3.1490e-02, -2.3746e-02, -2.1201e-02, -6.0342e-03,\n",
      "         -8.0623e-03,  3.5857e-02, -5.4131e-03,  1.8946e-03,  1.5363e-02,\n",
      "          3.4550e-02,  2.5643e-02,  3.6226e-03, -3.0600e-02,  2.8123e-02,\n",
      "          2.8104e-04, -2.9959e-02, -3.8663e-02, -3.5334e-02, -2.4602e-02,\n",
      "         -1.6836e-02,  4.0302e-03, -3.7967e-02, -4.5540e-03,  1.8477e-02,\n",
      "         -2.9148e-03,  3.7290e-03, -3.1157e-02, -5.5159e-03, -1.3205e-02,\n",
      "          1.2883e-02, -1.7810e-02,  3.6638e-02,  1.4800e-02, -2.1958e-02,\n",
      "         -1.9848e-02,  3.8146e-02,  1.7140e-02, -2.8204e-02, -2.4766e-02,\n",
      "         -4.5583e-03, -1.6183e-02, -2.6213e-03, -5.4063e-03, -1.3190e-02,\n",
      "         -2.9715e-02,  3.4327e-02, -1.6419e-02, -3.8755e-02,  3.7239e-03,\n",
      "          1.8309e-02, -9.1332e-03,  4.5137e-03,  3.1549e-02,  1.5902e-02,\n",
      "         -2.6879e-02,  1.4344e-02,  1.7822e-02, -4.2050e-03,  3.0253e-02,\n",
      "          2.7774e-02,  3.0566e-02,  2.5734e-03,  3.6196e-02,  8.0330e-03,\n",
      "          3.0069e-02, -3.1368e-02, -2.0340e-02,  6.2712e-03,  2.1229e-02,\n",
      "          2.5802e-02, -4.6339e-03, -1.2066e-02,  1.1567e-02,  3.9819e-02,\n",
      "          2.0767e-02, -3.2914e-02,  3.3971e-02,  3.7048e-02, -3.6717e-02,\n",
      "         -9.7828e-03, -2.8239e-02, -2.4951e-03,  3.9593e-02, -3.7753e-03,\n",
      "          3.0786e-02, -9.9337e-03, -2.7123e-02, -3.1310e-02, -8.6532e-03,\n",
      "         -3.7035e-02, -1.8399e-03, -1.6827e-02, -1.4361e-02,  3.4282e-02,\n",
      "         -1.0939e-03, -3.9889e-03, -1.9497e-02,  1.1557e-03,  1.2589e-02,\n",
      "          2.5135e-04,  2.8800e-02,  3.6992e-02,  1.2447e-03, -1.0996e-02,\n",
      "         -2.5918e-02, -3.9150e-02,  6.3548e-03, -1.0522e-02, -2.5176e-02,\n",
      "         -1.4266e-02, -5.2002e-03, -1.3104e-04, -1.1708e-02, -7.6065e-03,\n",
      "          1.7656e-03, -3.8375e-02,  5.7794e-03, -1.2642e-02,  2.2310e-02,\n",
      "          1.1433e-05,  1.6927e-02, -1.3128e-02, -2.7023e-02,  1.0345e-02,\n",
      "         -7.0073e-03, -3.6313e-02, -3.7739e-02,  3.7177e-02,  2.4579e-02,\n",
      "         -2.3190e-02, -2.8404e-02, -3.0723e-03,  3.9267e-02,  1.8291e-02,\n",
      "          2.9849e-02, -1.4753e-02, -3.6519e-03, -7.7260e-03, -2.8291e-02,\n",
      "          2.6955e-02,  3.3298e-03,  2.8714e-02, -2.6844e-03,  2.5620e-02,\n",
      "          3.8973e-03,  2.6369e-02, -3.3334e-02,  3.5527e-02, -3.3284e-02,\n",
      "         -2.9445e-03,  3.3128e-02,  1.4300e-02,  1.0210e-02,  2.8388e-02,\n",
      "         -2.3722e-02,  6.3328e-03,  9.5833e-03, -2.0387e-02, -2.9774e-02,\n",
      "         -4.9458e-04, -3.0407e-02,  1.1687e-03,  1.7317e-02, -9.5150e-03,\n",
      "          3.6046e-02,  3.1667e-02,  1.1582e-02, -1.9120e-02,  2.3023e-02,\n",
      "          6.6932e-03, -3.2943e-02, -1.7434e-03, -2.7453e-02, -7.4599e-03,\n",
      "          1.7203e-02,  2.8664e-02, -3.5626e-02,  5.6850e-03,  1.9216e-02,\n",
      "          6.1818e-03, -2.6082e-02, -3.1776e-02,  2.0778e-02, -2.0639e-02,\n",
      "          3.4606e-02,  2.4488e-02,  1.7982e-02, -4.3884e-03,  1.4561e-03,\n",
      "          2.9036e-02, -2.6871e-03,  3.9383e-02, -1.9265e-02, -3.7876e-02,\n",
      "         -3.2572e-02, -1.0612e-02,  1.4313e-02,  1.2621e-02,  1.8949e-02,\n",
      "         -2.7595e-02,  3.3415e-02, -3.3662e-02,  3.7675e-03,  1.6395e-02,\n",
      "          3.2039e-02,  3.0447e-02,  2.4171e-02, -3.7808e-02, -2.8825e-02,\n",
      "         -3.9497e-02,  3.0399e-02, -1.8979e-02, -1.8482e-02, -1.6453e-02,\n",
      "         -3.6477e-02,  2.4401e-02,  3.1023e-02,  1.5566e-03, -3.2091e-02,\n",
      "         -1.5927e-02, -3.1189e-03, -1.9781e-02, -1.5894e-02,  1.2083e-02,\n",
      "         -4.9732e-03,  1.2071e-02,  1.3881e-02,  2.0947e-02,  3.3885e-02,\n",
      "         -3.5838e-02,  2.3770e-02, -1.0222e-02, -9.6308e-03, -3.9552e-02,\n",
      "          3.6265e-02, -1.3752e-03,  3.8276e-02,  1.6876e-02,  2.0604e-02,\n",
      "          2.8564e-02,  2.1005e-02,  8.4364e-03,  3.8176e-02,  3.4980e-02,\n",
      "          1.6468e-02,  2.5714e-02, -2.1644e-02,  2.4581e-02,  3.3263e-03,\n",
      "         -1.6940e-02,  1.4881e-02, -3.5216e-02,  2.6275e-02,  3.5260e-04,\n",
      "          1.4400e-04,  1.1520e-02, -1.9575e-02, -2.2843e-02, -3.3513e-02,\n",
      "          2.1836e-02,  3.4034e-02, -2.5626e-02, -1.8957e-02,  1.7226e-02,\n",
      "          3.7587e-02, -3.5156e-02, -5.1523e-03, -3.1798e-02, -6.3338e-03,\n",
      "          2.4855e-02,  2.3889e-02, -1.7155e-02,  3.8747e-02, -2.8059e-03,\n",
      "         -3.7087e-02, -2.4335e-03,  3.9575e-02, -2.7267e-03, -2.9010e-02,\n",
      "         -3.9304e-02, -3.1526e-02, -5.2251e-03, -2.2102e-02, -3.7596e-02,\n",
      "          2.1397e-02,  2.1334e-02,  9.0118e-04, -2.1499e-02,  1.3564e-02,\n",
      "         -3.8815e-02,  1.5691e-02, -2.1144e-02, -8.7967e-03, -8.9429e-03,\n",
      "          2.6615e-02,  3.6207e-02,  3.9802e-02, -2.0067e-02,  3.8804e-02,\n",
      "         -2.2771e-02, -2.9458e-02, -1.8601e-02,  2.1164e-02, -9.0370e-03,\n",
      "          1.5951e-02,  2.5004e-02,  2.5528e-03,  1.7368e-02,  3.7344e-02,\n",
      "          8.8276e-03, -3.9715e-02, -3.8733e-02,  1.4904e-02, -3.2845e-02,\n",
      "          2.8831e-02,  2.5335e-02, -3.3358e-02,  3.6646e-03,  2.1582e-02,\n",
      "          3.8665e-02, -2.5664e-02,  3.5063e-02, -2.1900e-02, -3.4099e-02,\n",
      "          3.2870e-02,  2.2962e-02, -3.8420e-02,  2.6814e-02,  2.9774e-02,\n",
      "         -3.5903e-02,  3.0843e-02,  2.2390e-02,  1.6238e-02, -5.4744e-03,\n",
      "         -7.6611e-03, -3.7980e-03, -1.1898e-03,  1.4448e-02, -3.4994e-02,\n",
      "          3.1231e-02,  1.5805e-02, -2.1730e-02, -1.7383e-02, -2.8462e-02,\n",
      "          3.7333e-02,  1.2582e-03,  3.9945e-02,  2.6946e-02, -3.0878e-02,\n",
      "          1.4546e-02, -1.2045e-02, -2.3632e-02,  1.7382e-02, -1.6500e-02,\n",
      "         -1.8880e-03,  3.3583e-02,  7.7446e-03, -1.8693e-02,  1.1559e-02,\n",
      "          2.9572e-02, -3.6440e-02, -1.0185e-02,  6.8329e-03,  1.2735e-02,\n",
      "          2.8257e-02, -3.1907e-03,  3.9875e-02,  3.5045e-02,  5.8968e-03,\n",
      "         -2.0358e-02,  2.4401e-02,  1.6580e-03,  3.6722e-02, -2.9966e-02,\n",
      "         -2.7582e-02, -2.7691e-03, -7.6878e-03, -3.0101e-02,  1.1246e-02,\n",
      "          1.5362e-02, -2.0655e-02, -7.6224e-03,  2.9254e-02, -4.3042e-04,\n",
      "         -3.9561e-02,  3.9615e-02, -3.8110e-02,  2.5818e-02,  2.0513e-02,\n",
      "          6.9163e-03,  2.4704e-02, -1.7777e-03, -1.2468e-02,  2.0203e-03,\n",
      "         -2.2367e-02, -4.8383e-04,  4.4470e-03, -1.5243e-02, -1.3947e-02,\n",
      "         -2.2661e-02,  2.2064e-02, -3.8657e-02, -6.8357e-03, -3.7349e-02,\n",
      "          1.6848e-02, -3.0671e-03, -2.9595e-03, -3.3947e-02,  3.0013e-02,\n",
      "          3.3664e-02,  6.4585e-03,  5.2569e-03,  3.5235e-02, -1.2985e-02,\n",
      "          1.7180e-02, -2.4728e-02,  1.5316e-02, -3.2738e-02, -3.0400e-03,\n",
      "          1.8929e-03, -2.2678e-02,  3.7856e-02,  1.6182e-02, -2.8978e-03,\n",
      "         -4.0022e-02,  9.6789e-03, -3.9702e-02,  6.5547e-03, -1.7226e-02,\n",
      "          5.6957e-03,  5.9570e-03, -2.0090e-03, -2.8106e-02,  3.4796e-02,\n",
      "         -1.9535e-02,  3.5127e-02, -2.5176e-02,  3.4068e-02,  1.1850e-02,\n",
      "          7.2925e-03,  3.1033e-02, -2.1118e-02,  6.8386e-03, -1.9117e-02,\n",
      "          9.9709e-03,  3.5003e-02,  1.9190e-02,  3.4617e-02, -1.4299e-02,\n",
      "          3.6931e-02,  3.7954e-02,  3.1821e-02,  3.7582e-03,  1.0346e-02,\n",
      "         -1.3952e-02,  2.9402e-02,  5.0599e-03,  2.4648e-02,  3.2168e-03,\n",
      "          3.7233e-02,  2.2614e-02,  1.9239e-02, -1.1833e-02,  3.4598e-02,\n",
      "         -2.5567e-02,  7.8660e-03,  1.8270e-02, -3.6590e-02, -2.4268e-02,\n",
      "         -1.6225e-02, -1.5258e-02, -3.9593e-02, -1.7745e-02, -3.3895e-02,\n",
      "          2.0717e-02, -8.7318e-03,  1.0554e-02, -3.9025e-02,  8.1657e-03,\n",
      "          2.9505e-02,  1.3703e-02, -3.5953e-02,  1.1200e-02,  1.7794e-02,\n",
      "          2.4517e-02, -3.3511e-02, -1.2332e-02,  2.8747e-02, -3.6373e-02,\n",
      "         -3.3338e-02,  3.5046e-02,  1.9234e-02, -1.6085e-02, -7.1798e-04,\n",
      "          1.4271e-02, -3.5333e-02,  5.4526e-03, -2.2097e-02, -2.5625e-02,\n",
      "          2.4114e-02, -1.7727e-02, -3.9940e-02,  3.9498e-02, -1.1231e-02,\n",
      "          2.8969e-02,  3.1021e-02, -1.7044e-02, -3.7269e-02, -1.2912e-02,\n",
      "          1.1298e-02,  3.4019e-02, -1.6569e-02, -4.0024e-02,  2.6128e-02,\n",
      "         -5.6884e-03,  1.3806e-02, -1.1410e-02, -3.4857e-02, -1.2739e-02]])\n",
      "fc.bias tensor([-0.0273])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before training:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1544/1544 [00:14<00:00, 105.56batch/s, loss=1.14]\n",
      "Epoch 2/3: 100%|██████████| 1544/1544 [00:14<00:00, 108.64batch/s, loss=1.14]\n",
      "Epoch 3/3: 100%|██████████| 1544/1544 [00:14<00:00, 108.82batch/s, loss=1.14]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "# Training Loop \n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    for inputs, labels in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"NaN or Inf loss detected\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # # Print gradients for debugging\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad and param.grad is not None:\n",
    "        #         print(f\"Epoch {epoch+1}, {name}, grad mean: {param.grad.mean().item()}, grad std: {param.grad.std().item()}\")\n",
    "        \n",
    "        \n",
    "        train_loader_tqdm.set_postfix(loss=np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After training:\n",
      "fc.weight tensor([[-6.6770e-02, -1.6219e-01, -1.9123e-01,  3.3264e-02, -5.5131e-02,\n",
      "          1.9856e-01, -7.4966e-02,  1.0415e-02,  4.2963e-03, -1.1996e-01,\n",
      "          1.4478e-04, -2.4387e-01, -4.0595e-02,  1.0008e-02, -4.5290e-03,\n",
      "         -6.7988e-02,  8.3328e-02,  2.5317e-02, -6.4764e-04,  2.5841e-01,\n",
      "          4.4663e-03, -1.9136e-02, -1.8745e-02, -2.1727e-02,  2.3597e-02,\n",
      "         -4.3400e-03,  1.4128e-02,  2.2811e-01,  3.1540e-02,  7.8307e-03,\n",
      "          5.5843e-04,  1.8418e-02,  1.7266e-01, -1.9064e-01,  8.6679e-02,\n",
      "          3.4993e-01, -1.3642e-02, -4.1345e-02,  4.5309e-02,  2.0498e-01,\n",
      "          3.1002e-02, -4.3140e-02, -2.1724e-01,  1.3784e-02, -2.0480e-02,\n",
      "         -3.6071e-02, -1.0989e-02, -5.0587e-02, -3.2193e-02, -6.1329e-02,\n",
      "          7.8805e-03, -4.2035e-01, -1.7160e-01, -4.7590e-02,  1.4113e-02,\n",
      "          2.6231e-02, -1.2820e-01, -7.5579e-02,  1.7730e-01,  1.9761e-01,\n",
      "          2.5551e-02, -1.4347e-02,  3.6029e-02,  1.4598e-02,  2.4404e-03,\n",
      "          5.4972e-02,  2.3774e-02, -1.6710e-02,  3.0039e-01, -2.9042e-01,\n",
      "         -8.2159e-02, -2.0554e-02, -2.7426e-02,  4.7418e-02, -8.0021e-03,\n",
      "         -4.2942e-03,  2.9436e-02,  2.2050e-02, -1.2892e-01,  1.9287e-01,\n",
      "          1.6558e-02,  2.9781e-02, -3.9697e-02,  3.4300e-02, -3.9062e-02,\n",
      "          1.4747e-01, -1.3868e-03, -4.1185e-02, -3.2496e-02, -2.2951e-01,\n",
      "         -5.7897e-03, -1.5083e-02, -3.6916e-02, -3.7989e-02, -3.5923e-01,\n",
      "          5.9594e-04,  2.5699e-01,  8.4606e-02,  3.0043e-01,  3.5569e-02,\n",
      "         -5.8389e-02, -4.3665e-02, -9.0204e-02, -2.0155e-03, -1.2223e-01,\n",
      "          5.1830e-01, -1.3731e-01, -3.7461e-01, -2.5292e-01,  1.5359e-02,\n",
      "          4.6209e-02,  2.3409e-01, -2.0391e-01, -3.0298e-02,  3.1290e-02,\n",
      "          4.4974e-01, -1.5169e-01, -1.9984e-02, -3.1052e-02,  1.7303e-01,\n",
      "          1.7385e-01,  4.1039e-03, -3.5266e-02, -4.5540e-03,  1.8477e-02,\n",
      "         -1.3410e-03,  2.3112e-02, -3.1155e-02,  2.2152e-01, -1.2672e-02,\n",
      "         -3.9862e-01, -6.0012e-03, -7.2100e-02,  4.1174e-02,  1.7478e-02,\n",
      "         -1.9847e-02,  5.5554e-02, -1.6596e-02, -1.2704e-01, -8.2273e-02,\n",
      "         -4.5544e-03, -1.9873e-02, -1.0341e-01, -6.6823e-03, -3.1774e-01,\n",
      "         -2.8463e-02,  5.0306e-01,  2.2381e-01, -3.4438e-02,  4.5449e-02,\n",
      "          1.1926e-01, -1.4793e-02, -5.6734e-02,  4.3170e-03,  2.0386e-02,\n",
      "         -1.9424e-01,  1.4635e-02,  1.7676e-02, -4.7928e-03,  3.7789e-01,\n",
      "          2.7772e-02,  3.0569e-02,  5.2880e-02,  3.8915e-02, -3.5344e-02,\n",
      "         -7.1912e-02, -2.8060e-02, -6.5236e-02,  7.7915e-02,  1.7926e-01,\n",
      "          6.5199e-02,  3.8336e-03, -1.8276e-01,  1.2727e-01, -2.6232e-03,\n",
      "          1.1820e-02, -3.7341e-02, -4.2636e-02,  3.9253e-02, -1.4528e-02,\n",
      "         -3.1823e-02,  7.1990e-02,  1.3838e-02,  8.8803e-02,  9.8124e-03,\n",
      "         -3.2829e-02, -1.0988e-01, -2.4142e-02, -1.5369e-01, -1.1115e-01,\n",
      "          1.5869e-02, -1.7491e-01,  2.0154e-02, -1.4379e-02,  2.1516e-02,\n",
      "         -8.3278e-02,  1.2777e-01,  2.1800e-01,  1.1415e-03,  1.2593e-02,\n",
      "         -6.2655e-02,  2.8757e-02,  1.3118e-02, -1.7819e-02, -1.0801e-02,\n",
      "          2.9710e-02,  1.0071e-01, -4.0210e-02, -4.1417e-02, -4.3479e-02,\n",
      "          2.9763e-01, -5.1982e-03, -1.2941e-04,  2.0180e-03, -1.7524e-01,\n",
      "          7.3312e-02, -2.9514e-02,  5.7829e-03,  1.0969e-01,  3.4629e-02,\n",
      "          1.8774e-01, -2.9099e-02,  3.1519e-01,  2.7462e-01, -3.4610e-02,\n",
      "         -1.7418e-02, -5.0577e-02, -6.0891e-02,  3.7175e-02, -9.0332e-02,\n",
      "         -3.4011e-02,  2.2802e-01, -3.0707e-03, -7.0781e-02,  2.1096e-02,\n",
      "         -1.3678e-01,  7.3687e-02,  1.0249e-02, -6.8936e-02, -1.2511e-01,\n",
      "          4.5689e-01,  3.3326e-03,  3.3314e-02, -1.2730e-03, -1.0341e-01,\n",
      "          1.0710e-01,  2.6373e-02,  1.5744e-01,  2.5972e-01, -9.2280e-02,\n",
      "          1.0531e-01,  6.9729e-02,  1.1530e-01, -8.2141e-03,  3.4245e-02,\n",
      "         -2.2238e-02,  3.1446e-01,  5.3887e-04,  2.7633e-01,  2.3644e-02,\n",
      "         -2.1597e-01,  4.7310e-03,  6.7989e-02, -1.1198e-01, -9.5150e-03,\n",
      "          5.7020e-02,  3.0078e-01,  2.4182e-01, -1.9121e-02, -2.8477e-01,\n",
      "         -7.6411e-02, -7.4610e-02, -4.4117e-01,  2.8655e-01, -7.4530e-03,\n",
      "          1.8849e-02,  2.1706e-02,  1.4849e-01,  2.8060e-02,  3.7629e-01,\n",
      "          5.1682e-02,  2.3746e-02, -1.1345e-01,  7.6097e-02,  1.0127e-01,\n",
      "         -4.2731e-02,  4.1506e-02,  2.2436e-01, -4.3918e-03, -2.3608e-02,\n",
      "          2.8186e-02, -3.5809e-03, -9.9612e-02,  1.8285e-01, -2.8477e-02,\n",
      "         -3.2576e-02,  1.2920e-01,  1.0008e-01, -1.8351e-01,  6.5488e-02,\n",
      "          2.1747e-01,  1.7173e-01, -3.3658e-02, -1.9661e-01,  1.2739e-02,\n",
      "          7.6608e-02,  3.0636e-02,  5.0817e-02, -3.7809e-02, -2.8823e-02,\n",
      "         -2.1171e-02,  1.0063e-01, -1.8979e-02, -2.5686e-02, -1.6456e-02,\n",
      "         -3.2243e-02, -1.3469e-02,  3.0387e-02, -7.5633e-02,  1.0851e-02,\n",
      "          5.5185e-02, -3.1186e-03,  1.3036e-01, -6.8246e-02, -1.8040e-01,\n",
      "         -4.9660e-03, -1.3858e-02, -7.8103e-02,  2.0950e-02,  5.8328e-03,\n",
      "         -2.5776e-01,  5.1443e-02, -3.4622e-01,  1.8286e-02, -3.9822e-02,\n",
      "         -8.3182e-03, -3.7711e-01,  6.7688e-02, -2.7151e-02, -2.0298e-01,\n",
      "          7.8714e-02, -3.8299e-01, -4.3324e-03, -6.3888e-03,  3.4948e-02,\n",
      "         -6.8278e-03, -8.5218e-02, -5.6030e-02, -6.2466e-02,  3.3257e-03,\n",
      "         -8.8051e-02, -2.2338e-01, -3.5217e-02,  2.6569e-02,  2.3230e-02,\n",
      "         -5.0460e-02,  9.4075e-03,  5.7067e-02,  1.1187e-01, -2.3627e-01,\n",
      "          6.4347e-02, -1.3168e-02, -7.4308e-02,  3.1942e-02,  2.5783e-01,\n",
      "         -1.2715e-01, -1.2066e-01, -1.8118e-01,  1.5717e-01,  1.5586e-01,\n",
      "          2.3657e-01,  8.2294e-03, -1.4099e-01,  2.2228e-02, -8.1539e-02,\n",
      "         -9.5305e-02,  5.4325e-02,  7.1466e-02, -5.2451e-02, -2.2418e-01,\n",
      "          1.4071e-01, -8.7206e-03, -3.3539e-03, -2.1523e-02,  7.3740e-02,\n",
      "         -3.5890e-02, -7.5265e-02,  1.7637e-01,  1.8902e-01, -1.4180e-02,\n",
      "          1.4718e-02,  1.8361e-03, -4.1788e-02, -1.4888e-01, -1.0456e-01,\n",
      "          2.3166e-02, -1.6740e-02,  1.1516e-02, -2.2319e-02, -7.7684e-02,\n",
      "         -8.3330e-02, -4.3038e-02, -7.5807e-02,  4.3768e-02, -1.3742e-02,\n",
      "          1.6554e-02, -3.0301e-02, -5.6320e-03,  5.8962e-02, -1.0198e-01,\n",
      "         -1.5356e-01, -9.3230e-03, -1.4921e-01,  5.3598e-02,  2.5589e-02,\n",
      "         -5.5130e-02, -1.6405e-02,  3.4261e-03, -5.4561e-02, -3.3207e-02,\n",
      "         -6.8325e-02,  9.6273e-02, -1.4453e-02, -1.3091e-01,  4.4390e-02,\n",
      "          4.7700e-02, -9.1503e-02, -4.9422e-02,  3.2980e-01, -3.6364e-02,\n",
      "          1.2615e-02, -9.3036e-02, -3.6673e-02,  2.3022e-02, -1.3629e-03,\n",
      "         -3.8007e-02,  2.7221e-02,  1.4609e-01,  2.5727e-01,  1.5899e-01,\n",
      "          2.9662e-01,  9.1806e-02, -5.7538e-01, -5.0628e-01, -2.8984e-02,\n",
      "          3.5877e-02,  1.2618e-03, -1.9962e-01,  4.6741e-01, -2.7500e-02,\n",
      "          1.4098e-02, -1.1368e-01, -2.5387e-01, -5.5656e-02, -1.8939e-02,\n",
      "         -1.6355e-01, -7.6618e-02,  3.2607e-01, -2.4280e-02, -1.6977e-01,\n",
      "          2.9577e-02,  2.0317e-01, -1.0181e-02,  3.1684e-01,  3.6199e-02,\n",
      "          2.8111e-02, -1.1039e-01,  1.7800e-02,  3.8534e-01, -2.4283e-02,\n",
      "         -1.6363e-01,  1.0922e-02, -9.0966e-03,  1.4504e-01, -6.1289e-02,\n",
      "         -2.7584e-02, -1.9382e-02,  2.2951e-01, -3.0099e-02, -2.4619e-01,\n",
      "          1.5362e-02, -2.1974e-02,  1.4791e-01, -2.0307e-01, -1.3985e-01,\n",
      "          4.1944e-02,  3.3250e-02,  1.1159e-02,  3.2225e-02,  2.9031e-01,\n",
      "         -1.9018e-01,  3.8072e-02,  2.4098e-01, -5.9193e-02,  1.6403e-01,\n",
      "         -1.7562e-01, -1.1374e-01, -7.1591e-03, -1.6439e-01,  1.7355e-01,\n",
      "          2.1404e-01,  3.9091e-02,  5.6177e-01, -1.1722e-01,  9.3258e-02,\n",
      "         -3.0181e-01,  2.3673e-01,  3.0974e-02, -4.1727e-01,  1.6039e-01,\n",
      "          2.9623e-02, -1.0982e-01, -2.6782e-01,  2.5522e-01,  3.0054e-01,\n",
      "         -2.3711e-01, -3.1677e-02, -4.9464e-02,  5.4505e-01,  2.3648e-03,\n",
      "          2.7043e-01, -2.6212e-01, -8.1702e-02, -8.4861e-02, -8.1153e-02,\n",
      "          1.2733e-01, -1.8949e-01,  3.8444e-01, -3.8963e-01, -2.0084e-02,\n",
      "          4.4807e-01, -3.5041e-01, -1.3645e-02, -3.6883e-01, -1.3427e-01,\n",
      "          2.7314e-01, -2.3057e-01,  3.4692e-01,  1.9226e-01, -8.1629e-02,\n",
      "         -1.3763e-01,  1.5588e-01,  1.6901e-01,  2.9294e-01,  2.1928e-01,\n",
      "         -2.4488e-01,  1.5414e-01, -5.8116e-02, -1.9721e-01, -1.1669e-02,\n",
      "          4.4852e-01,  3.3742e-01,  4.4659e-01, -3.5601e-01,  2.4981e-01,\n",
      "          6.5979e-02, -3.2198e-02, -2.3403e-01,  2.8798e-01, -2.3393e-01,\n",
      "          3.5722e-01, -1.5912e-01, -3.8015e-01, -2.2425e-01,  6.8742e-02,\n",
      "          2.8972e-01, -8.3343e-03, -1.3695e-01,  1.7453e-01, -3.1551e-02,\n",
      "         -8.2175e-02, -1.6973e-02, -5.1872e-02, -1.4802e-02,  1.3056e-01,\n",
      "          1.3940e-01, -6.0017e-02,  9.2351e-03,  2.7293e-01, -1.0879e-01,\n",
      "          3.8627e-02, -1.4426e-02, -3.5375e-02,  2.6818e-02, -4.1562e-03,\n",
      "         -5.4515e-03, -4.2372e-02,  4.7708e-02,  4.4034e-01,  4.0410e-01,\n",
      "         -3.4656e-02, -1.6239e-02,  2.2177e-02,  5.1937e-01, -4.1490e-03,\n",
      "         -2.3982e-01,  2.4708e-02, -1.9508e-01,  2.0328e-01, -1.3646e-01,\n",
      "         -2.2828e-02, -1.0160e-02,  3.2972e-03, -1.9167e-02, -4.4623e-02,\n",
      "          7.2756e-02, -2.5666e-02, -1.2902e-02, -6.1149e-02, -2.0579e-02,\n",
      "         -1.1852e-01,  2.0165e-02, -2.1274e-02,  3.8464e-02, -1.0050e-02,\n",
      "          3.7952e-01,  4.8011e-02,  4.2505e-03, -8.8607e-02,  1.2286e-01]])\n",
      "fc.bias tensor([-0.0180])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAfter training:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "losses = []\n",
    "unscaled_losses = []\n",
    "test_loader_tqdm = tqdm(test_loader, desc='Evaluating', unit='batch')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_tqdm:\n",
    "        # Compute scaled outputs\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate scaled loss\n",
    "        loss = criterion(outputs.reshape(-1, 1), labels.reshape(-1, 1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Inverse transform the scaled outputs and labels to get unscaled values\n",
    "        unscaled_outputs = target_scaler.inverse_transform(outputs.numpy().reshape(-1, 1))\n",
    "        unscaled_labels = target_scaler.inverse_transform(labels.numpy().reshape(-1, 1))\n",
    "\n",
    "        # Ensure the shapes are correct for unscaled loss calculation\n",
    "        unscaled_outputs_tensor = torch.tensor(unscaled_outputs, dtype=torch.float32).reshape(-1)\n",
    "        unscaled_labels_tensor = torch.tensor(unscaled_labels, dtype=torch.float32).reshape(-1)\n",
    "\n",
    "        # Calculate unscaled loss\n",
    "        unscaled_loss = criterion(unscaled_outputs_tensor, unscaled_labels_tensor)\n",
    "        unscaled_losses.append(unscaled_loss.item())\n",
    "\n",
    "# Calculate mean losses\n",
    "mean_loss = np.mean(losses)\n",
    "mean_unscaled_loss = np.mean(unscaled_losses)\n",
    "\n",
    "print(f'Test Loss: {mean_loss}')\n",
    "print(f'Unscaled Test Loss: {mean_unscaled_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler\n",
    "Evaluating: 100%|██████████| 386/386 [00:02<00:00, 159.17batch/s]\n",
    "Test Loss: 0.0031778891372756485\n",
    "Unscaled Test Loss: 939906541196.6011\n",
    "\n",
    "\n",
    "# StandardScaler\n",
    "Evaluating: 100%|██████████| 386/386 [00:02<00:00, 164.46batch/s]\n",
    "Test Loss: 0.8869521320083328\n",
    "Unscaled Test Loss: 783619205189.9689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_row = dft.iloc[[random.choice(range(len(df)))]]\n",
    "\n",
    "X = random_row.drop(columns=[target_col] + special_cols).astype(float)\n",
    "y = random_row[target_col]\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "pred = model(X_tensor).detach().numpy()\n",
    "pred_it = target_scaler.inverse_transform(pred).item()\n",
    "\n",
    "y_it = target_scaler.inverse_transform(y_tensor.reshape(-1, 1)).item()\n",
    "\n",
    "pred_it, y_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
