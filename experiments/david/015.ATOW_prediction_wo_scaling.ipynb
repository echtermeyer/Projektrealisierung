{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\n"
     ]
    }
   ],
   "source": [
    "# working dir\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root_dir = os.path.dirname(os.path.dirname(cwd))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display Dataframe (with scrollbars)\n",
    "def ddf(df, max_height=500, max_width=1500):\n",
    "    \"\"\"\n",
    "    Display a pandas DataFrame with horizontal and vertical scrollbars in a Jupyter notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to display.\n",
    "    max_height (int): The maximum height of the scrollable area in pixels.\n",
    "    max_width (int): The maximum width of the scrollable area in pixels.\n",
    "    \"\"\"\n",
    "    style = f\"\"\"\n",
    "    <style>\n",
    "    .scrollable-dataframe {{\n",
    "        max-height: {max_height}px;\n",
    "        max-width: {max_width}px;\n",
    "        overflow: auto;\n",
    "        display: inline-block;\n",
    "        position: relative;\n",
    "    }}\n",
    "    .scrollable-dataframe thead th {{\n",
    "        position: sticky;\n",
    "        top: 0;\n",
    "        background-color: white;\n",
    "        z-index: 1;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    html = style + df.to_html(classes='scrollable-dataframe')\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_888\\3172802451.py:2: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(\"../../src/data/ABCD_tripfiles_preprocessed.csv\"),\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_888\\3172802451.py:3: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24,25,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(\"../../src/data/MNOP_tripfiles_preprocessed.csv\"),\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_888\\3172802451.py:4: DtypeWarning: Columns (26,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv(\"../../src/data/ZYXW_tripfiles_preprocessed.csv\"),\n"
     ]
    }
   ],
   "source": [
    "main = pd.concat([\n",
    "    pd.read_csv(\"../../src/data/ABCD_tripfiles_preprocessed.csv\"),\n",
    "    pd.read_csv(\"../../src/data/MNOP_tripfiles_preprocessed.csv\"),\n",
    "    pd.read_csv(\"../../src/data/ZYXW_tripfiles_preprocessed.csv\"),\n",
    "])\n",
    "\n",
    "meta_cols = ['flight_id', 'id', 'creation_time', 'airline_code', 'flight_date', 'action_name',]\n",
    "action_cols = ['departureAirport', 'departureTime', 'arrivalAirport', 'arrivalTime', 'aircraftRegistration', 'aircraftSubtype', 'aircraftVersion',]\n",
    "\n",
    "main = main.loc[\n",
    "    main[\"action_name\"] == \"CalculateWeightAndTrimAction\",\n",
    "    meta_cols + action_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable future deprecation warning of .fillna() method\n",
    "\n",
    "def fill_na_within_group(group):\n",
    "    group[action_cols] = group[action_cols].fillna(method='bfill')\n",
    "    group[action_cols] = group[action_cols].fillna(method='ffill')\n",
    "    return group\n",
    "\n",
    "main = main.groupby(\"flight_id\").apply(fill_na_within_group).reset_index(drop=True) # replaces NaN values with the values from the previous or next row within the same flight\n",
    "main.dropna(inplace=True) # drops 203828 rows - these are likely flights without an ASMMsgProcessor action\n",
    "\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_year(date_str):\n",
    "    date_part, time_part = date_str.split('T')\n",
    "    year, month, day = date_part.split('-')\n",
    "    hours, minutes, seconds = time_part.split(':')\n",
    "\n",
    "    # Correct year\n",
    "    if len(year) != 4:\n",
    "        year = year[1:]\n",
    "    \n",
    "    # Correct the minutes\n",
    "    if len(minutes) > 2:\n",
    "        minutes = minutes[:2]\n",
    "\n",
    "    # Correct the seconds\n",
    "    if len(seconds) > 6:\n",
    "        seconds = \"00.000Z\"\n",
    "    \n",
    "    date_part = f\"{year}-{month}-{day}\"\n",
    "    time_part = f\"{hours}:{minutes}:{seconds}\"\n",
    "    date_str = f\"{date_part}T{time_part}\"\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "# Fix year in departureTime and arrivalTime\n",
    "main[\"departureTime\"] = main[\"departureTime\"].apply(lambda x: correct_year(str(x)))\n",
    "main[\"arrivalTime\"] = main[\"arrivalTime\"].apply(lambda x: correct_year(str(x)))\n",
    "\n",
    "\n",
    "main[\"creation_time\"] = pd.to_datetime(main[\"creation_time\"])\n",
    "main[\"departureTime\"] = pd.to_datetime(main[\"departureTime\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "main[\"arrivalTime\"] = pd.to_datetime(main[\"arrivalTime\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "main[\"minutes_till_dep\"] = (main[\"departureTime\"] - main[\"creation_time\"]).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwata = pd.concat([\n",
    "    pd.read_csv(\"../../src/data/extracted/abcd_CalculateWeightAndTrimAction.csv\"), \n",
    "    pd.read_csv(\"../../src/data/extracted/mnop_CalculateWeightAndTrimAction.csv\"), \n",
    "    pd.read_csv(\"../../src/data/extracted/zyxw_CalculateWeightAndTrimAction.csv\")\n",
    "    ])\n",
    "\n",
    "cwata = cwata[[\n",
    "    'id', 'START_WI_weight', \n",
    "    'DO_WI_weight', 'PAX_WI_weight', 'TOTAL_DEADLOAD_WI_weight', 'TOTAL_LOAD_WI',\n",
    "    'TOTAL_TRAFFIC_LOAD', 'AZFW', 'ATOW', 'ALAW', 'ATXW',\n",
    "    'LIZFW', 'LITOW', 'LILAW',\n",
    "    'DEADLOAD_MAC', 'UNDERLOAD',\n",
    "    'ALLOWED_TOW', 'ALLOWED_ZFW', 'ALLOWED_LAW',\n",
    "    'ALLOWED_TXW',\n",
    "    'ESTIMATED_TRAFFIC_LOAD', 'ESTIMATED_ZFW',\n",
    "    'DELTA_ZFW'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(cwata, main, on=\"id\", how=\"left\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.sort_values(by=[\"flight_id\", \"creation_time\", \"id\"], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "azfw_all = df.groupby('flight_id')['ATOW'].last()\n",
    "azfw_all = azfw_all.to_dict()\n",
    "\n",
    "df[\"target_ATOW\"] = df[\"flight_id\"].map(azfw_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.468920e+05\n",
       "mean    -2.048361e+03\n",
       "std      7.825867e+05\n",
       "min     -2.189566e+07\n",
       "25%      0.000000e+00\n",
       "50%      2.600000e+01\n",
       "75%      1.722000e+03\n",
       "max      1.708310e+07\n",
       "Name: delta, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"delta\"] = df[\"target_ATOW\"] - df[\"ATOW\"]\n",
    "df[\"delta\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cols = ['flight_id', 'creation_time', 'id', 'action_name', \"departureTime\", \"arrivalTime\", \"delta\"]\n",
    "cat_cols = [\"airline_code\", \"departureAirport\", \"arrivalAirport\", \"aircraftRegistration\", \"aircraftSubtype\", \"aircraftVersion\"]\n",
    "target_col = \"target_ATOW\"\n",
    "num_cols = list(set(df.columns) - set(cat_cols) - set(special_cols) - {target_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col] + cat_cols + special_cols)\n",
    "y = df[target_col]\n",
    "\n",
    "# Change one hot bools to floats\n",
    "X = X.astype(float)\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Regression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Regression, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc(self.bn(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_tensor.shape[1] # 620\n",
    "model = SimpleNN(input_size)\n",
    "# model = Regression(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/1544 [00:00<?, ?batch/s]c:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/50: 100%|█████████▉| 1537/1544 [00:19<00:00, 83.35batch/s, loss=8.98e+4]c:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/50: 100%|██████████| 1544/1544 [00:19<00:00, 79.34batch/s, loss=8.98e+4]\n",
      "Epoch 2/50: 100%|██████████| 1544/1544 [00:20<00:00, 76.85batch/s, loss=8.65e+4]\n",
      "Epoch 3/50: 100%|██████████| 1544/1544 [00:20<00:00, 76.23batch/s, loss=8.6e+4] \n",
      "Epoch 4/50: 100%|██████████| 1544/1544 [00:20<00:00, 76.04batch/s, loss=8.64e+4]\n",
      "Epoch 5/50: 100%|██████████| 1544/1544 [00:20<00:00, 75.76batch/s, loss=8.64e+4]\n",
      "Epoch 6/50: 100%|██████████| 1544/1544 [00:21<00:00, 73.44batch/s, loss=8.63e+4]\n",
      "Epoch 7/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.07batch/s, loss=8.63e+4]\n",
      "Epoch 8/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.59batch/s, loss=8.62e+4]\n",
      "Epoch 9/50: 100%|██████████| 1544/1544 [00:21<00:00, 72.82batch/s, loss=8.6e+4] \n",
      "Epoch 10/50: 100%|██████████| 1544/1544 [00:21<00:00, 71.69batch/s, loss=8.61e+4]\n",
      "Epoch 11/50: 100%|██████████| 1544/1544 [00:21<00:00, 71.02batch/s, loss=8.6e+4] \n",
      "Epoch 12/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.66batch/s, loss=8.6e+4] \n",
      "Epoch 13/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.98batch/s, loss=8.59e+4]\n",
      "Epoch 14/50: 100%|██████████| 1544/1544 [00:20<00:00, 75.21batch/s, loss=8.61e+4]\n",
      "Epoch 15/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.61batch/s, loss=8.58e+4]\n",
      "Epoch 16/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.02batch/s, loss=8.58e+4]\n",
      "Epoch 17/50: 100%|██████████| 1544/1544 [00:21<00:00, 72.84batch/s, loss=8.58e+4]\n",
      "Epoch 18/50: 100%|██████████| 1544/1544 [00:20<00:00, 73.85batch/s, loss=8.57e+4]\n",
      "Epoch 19/50: 100%|██████████| 1544/1544 [00:20<00:00, 74.52batch/s, loss=8.57e+4]\n",
      "Epoch 20/50: 100%|██████████| 1544/1544 [00:20<00:00, 73.59batch/s, loss=8.57e+4]\n",
      "Epoch 21/50: 100%|██████████| 1544/1544 [00:21<00:00, 72.37batch/s, loss=8.57e+4]\n",
      "Epoch 22/50:  77%|███████▋  | 1194/1544 [00:16<00:04, 71.76batch/s, loss=8.6e+4] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader_tqdm:\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# if torch.isnan(loss) or torch.isinf(loss):\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#     print(\"NaN or Inf loss detected\")\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\Desktop\\Main\\03_Uni\\WWI21DSA\\02_Vorlesungen\\06_Projektrealisierung\\Projektrealisierung\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()  # Using Mean Absolute Error as criterion to avoid inf loss when not scaling\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "# Training Loop \n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # print(\"params in epoch\", epoch+1)\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(name, param.data)\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    for inputs, labels in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # if torch.isnan(loss) or torch.isinf(loss):\n",
    "        #     print(\"NaN or Inf loss detected\")\n",
    "        #     break\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # # Print gradients for debugging\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad and param.grad is not None:\n",
    "        #         print(f\"Epoch {epoch+1}, {name}, grad mean: {param.grad.mean().item()}, grad std: {param.grad.std().item()}\")\n",
    "        \n",
    "        \n",
    "        train_loader_tqdm.set_postfix(loss=np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "losses = []\n",
    "unscaled_losses = []\n",
    "test_loader_tqdm = tqdm(test_loader, desc='Evaluating', unit='batch')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_tqdm:\n",
    "        # Compute scaled outputs\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate scaled loss\n",
    "        loss = criterion(outputs.reshape(-1, 1), labels.reshape(-1, 1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Calculate mean losses\n",
    "mean_loss = np.mean(losses)\n",
    "\n",
    "print(f'Test Loss: {mean_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler & MSE\n",
    "Evaluating: 100%|██████████| 386/386 [00:02<00:00, 159.17batch/s]\n",
    "Test Loss: 0.0031778891372756485\n",
    "Unscaled Test Loss: 939906541196.6011\n",
    "\n",
    "#### StandardScaler & MSE\n",
    "Evaluating: 100%|██████████| 386/386 [00:02<00:00, 164.46batch/s]\n",
    "Test Loss: 0.8869521320083328\n",
    "Unscaled Test Loss: 783619205189.9689\n",
    "\n",
    "--> not only does the loss suck, it doesnt decrease over the epochs either\n",
    "\n",
    "#### No Scaler & MAE \n",
    "Evaluating: 100%|██████████| 386/386 [00:00<00:00, 551.40batch/s]\n",
    "Test Loss: 375176577.83937824\n",
    "Unscaled Test Loss: nan\n",
    "\n",
    "--> still sucks but at least the loss decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[target_col] + cat_cols + special_cols).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_row = df.iloc[[random.choice(range(len(df)))]]\n",
    "\n",
    "X = random_row.drop(columns=[target_col] + cat_cols + special_cols + [\"delta\"]).astype(float)\n",
    "y = random_row[target_col]\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "pred = model(X_tensor).detach().numpy()\n",
    "\n",
    "# pred_it = target_scaler.inverse_transform(pred).item()\n",
    "# y_it = target_scaler.inverse_transform(y_tensor.reshape(-1, 1)).item()\n",
    "# pred_it, y_it\n",
    "\n",
    "pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
